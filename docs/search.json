[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Shelby Kimmel’s blog sit for machine learning"
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "lecture-notes/text-generation.html",
    "href": "lecture-notes/text-generation.html",
    "title": "Text Generation",
    "section": "",
    "text": "Open these notes in Google Colab\nOpen the live version in Google Colab"
  },
  {
    "objectID": "lecture-notes/text-generation.html#text-generation",
    "href": "lecture-notes/text-generation.html#text-generation",
    "title": "Text Generation",
    "section": "Text Generation",
    "text": "Text Generation\nIn this set of notes, we’ll see a simple example of how to design and train models that perform text generation. Large language models (often called chatbots) are one familiar technology that uses text generation, while autocomplete features on websites and your devices are another. The text generation task is:\n\nGiven a text prompt, return a sequence of text that appears realistic as a follow-up to that prompt.\n\nExcept for a brief foray into unsupervised learning, almost all of our attention in this course has been focused on prediction problems. At first glance, it may not appear that text generation involves any prediction at all. However, modern approaches to text generation rely fundamentally on supervised learning through the framework of next token prediction."
  },
  {
    "objectID": "lecture-notes/text-generation.html#next-token-prediction",
    "href": "lecture-notes/text-generation.html#next-token-prediction",
    "title": "Text Generation",
    "section": "Next Token Prediction",
    "text": "Next Token Prediction\nThe next token prediction problem is to predict a single token in terms of previous tokens. A token is a single “unit” of text. What counts as a unit is somewhat flexible. In some cases, each token might be a single character: “a” is a token, “b” is a token, etc. In other cases, each token might be a word. Many modern models do something in between and let tokens represent common short sequences of characters using byte-pair encoding.\nFor this set of lecture notes, we’re going to treat words and punctuation as tokens. The next token prediction problem is:\n\nGiven a sequence of tokens, predict the next token in the sequence.\n\nFor example, suppose that our sequence of tokens is\n\n“A computer science student”\n\nWe’d like to predict the next token in the sequence. Some likely candidates:\n\n“is”\n“codes”\n“will”\n\netc. On the other hand, some unlikely candidates:\n\n“mango”\n“grassy”\n“tree”\n\nSo, we can think of this as a prediction, even a classification problem: the sequence “A computer science student” might be classified as “the category of sequences that are likely to be followed by the word is”.\nOnce we have trained a model, the text generation task involves asking that model to make predictions, using those predictions to form new tokens, and then feeding those new tokens into the model again to get even more new tokens, etc.\n\nimport pandas as pd\nimport torch\nimport numpy as np\nimport string\nfrom torchsummary import summary\nfrom torchtext.vocab import build_vocab_from_iterator\nimport torch.utils.data as data\nfrom torch import nn\nfrom torch.nn.functional import relu\nimport re\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
  },
  {
    "objectID": "lecture-notes/text-generation.html#our-task",
    "href": "lecture-notes/text-generation.html#our-task",
    "title": "Text Generation",
    "section": "Our Task",
    "text": "Our Task\nToday, we are going to see whether we can teach an algorithm to understand and reproduce the pinnacle of cultural achievement; the benchmark against which all art is to be judged; the mirror that reveals to humany its truest self. I speak, of course, of Star Trek: Deep Space Nine.\n\n\n\n\n\n\nIn particular, we are going to attempt to teach a neural network to generate episode scripts. This a text generation task: after training, our hope is that our model will be able to create scripts that are reasonably realistic in their appearance.\n\n## miscellaneous data cleaning\n\nstart_episode = 20\nnum_episodes = 25\n\nurl = \"https://github.com/PhilChodrow/PIC16B/blob/master/datasets/star_trek_scripts.json?raw=true\"\nstar_trek_scripts = pd.read_json(url)\n\ncleaned = star_trek_scripts[\"DS9\"].str.replace(\"\\n\\n\\n\\n\\n\\nThe Deep Space Nine Transcripts -\", \"\")\ncleaned = cleaned.str.split(\"\\n\\n\\n\\n\\n\\n\\n\").str.get(-2)\ntext = \"\\n\\n\".join(cleaned[start_episode:(start_episode + num_episodes)])\nfor char in ['\\xa0', 'à', 'é', \"}\", \"{\"]:\n    text = text.replace(char, \"\")\n\nThis is a long string of text.\n\nlen(text)\n\n788662\n\n\nHere’s what it looks like when printed:\n\nprint(text[0:500])\n\n  Last\ntime on Deep Space Nine.  \nSISKO: This is the emblem of the Alliance for Global Unity. They call\nthemselves the Circle. \nO'BRIEN: What gives them the right to mess up our station? \nODO: They're an extremist faction who believe in Bajor for the\nBajorans. \nSISKO: I can't loan you a Starfleet runabout without knowing where you\nplan on taking it. \nKIRA: To Cardassia Four to rescue a Bajoran prisoner of war. \n(The prisoners are rescued.) \nKIRA: Come on. We have a ship waiting. \nJARO: What you \n\n\nThe string in raw form doesn’t look quite as nice:\n\ntext[0:100]\n\n'  Last\\ntime on Deep Space Nine.  \\nSISKO: This is the emblem of the Alliance for Global Unity. They c'"
  },
  {
    "objectID": "lecture-notes/text-generation.html#data-prep",
    "href": "lecture-notes/text-generation.html#data-prep",
    "title": "Text Generation",
    "section": "Data Prep",
    "text": "Data Prep\n\nTokenization\nIn order to feed this string into a language model, we are going to need to split it into tokens. For today, we are going to treat punctuation, newline \\n characters, and words as tokens. Here’s a hand-rolled tokenizer that achieves this:\n\ndef tokenizer(text):\n    \n    # empty list of tokens\n    out = []\n    \n    # start by splitting into lines and candidate tokens\n    # candidate tokens are separated by spaces\n    L = [s.split() for s in text.split(\"\\n\")]\n    \n    # for each list of candidate tokens \n    for line in L:\n        # scrub punctuation off beginning and end, adding to out as needed\n        for token in line:             \n            while (len(token) &gt; 0) and (token[0] in string.punctuation):\n                out.append(token[0])\n                token = token[1:]\n            \n            stack = []\n            while (len(token) &gt; 0) and (token[-1] in string.punctuation):\n                stack.insert(0, token[-1]) \n                token = token[:-1]\n            \n            out.append(token)\n            if len(stack) &gt; 0:\n                out += stack\n        out += [\"\\n\"]\n    \n    # return the list of tokens, except for the final \\n\n    return out[:-1]\n\nHere’s this tokenizer in action:\n\ntokenizer(\"Last\\ntime on Deep Space Nine. \\n SISKO: This\")\n\n['Last',\n '\\n',\n 'time',\n 'on',\n 'Deep',\n 'Space',\n 'Nine',\n '.',\n '\\n',\n 'SISKO',\n ':',\n 'This']\n\n\nLet’s tokenize the entire string:\n\ntoken_seq = tokenizer(text)\n\n\n\nAssembling the Data Set\nWhat we’re now going to do is assemble the complete list of tokens into a series of predictor sequences and target tokens. The code below does this. The WINDOW controls how long each predictor sequence should be, and the STEP controls how many sequences we extract. A STEP of 1 would be all possible sequences. I’ve increased the STEP to 50 to reduce the size of our data for practical purposes.\n\nseq_len = 10\nSTEP = 1\n\npredictors = []\ntargets    = []\n\nfor i in range(0, len(token_seq) - seq_len - 1, STEP):\n    predictors.append(token_seq[i:(i+seq_len)])\n    targets.append(token_seq[seq_len+i])\n\nHere’s how this looks:\n\nfor i in range(100, 105):\n    print(predictors[i], end = \"\")\n    print(\" | \" + targets[i])\n\n[')', '\\n', 'KIRA', ':', 'Come', 'on', '.', 'We', 'have', 'a'] | ship\n['\\n', 'KIRA', ':', 'Come', 'on', '.', 'We', 'have', 'a', 'ship'] | waiting\n['KIRA', ':', 'Come', 'on', '.', 'We', 'have', 'a', 'ship', 'waiting'] | .\n[':', 'Come', 'on', '.', 'We', 'have', 'a', 'ship', 'waiting', '.'] | \n\n['Come', 'on', '.', 'We', 'have', 'a', 'ship', 'waiting', '.', '\\n'] | JARO\n\n\nOur next task is to convert all these tokens into unique integers, just like we did for text classification (because this basically is still text classification). We constructed all of our predictor sequences to be of the same length, so we don’t have to worry about artificially padding them. This makes our task of preparing the data set much easier.\n\nvocab = build_vocab_from_iterator(iter(predictors), specials=[\"&lt;unk&gt;\"])\nvocab.set_default_index(vocab[\"&lt;unk&gt;\"])\n\nX = [vocab(x) for x in predictors]\ny = vocab(targets)\n\n## here's how our data looks now: \n\nfor i in range(100, 105):\n    print(X[i], end = \"\")\n    print(\" | \" + str(y[i]))\n\n[19, 1, 28, 3, 302, 22, 2, 83, 23, 10] | 161\n[1, 28, 3, 302, 22, 2, 83, 23, 10, 161] | 448\n[28, 3, 302, 22, 2, 83, 23, 10, 161, 448] | 2\n[3, 302, 22, 2, 83, 23, 10, 161, 448, 2] | 1\n[302, 22, 2, 83, 23, 10, 161, 448, 2, 1] | 399\n\n\nSince our predictors are all in the same shape, we can go ahead and immediately construct the tensors and data sets we need:\n\nn = len(X)\n\nX = torch.tensor(X, dtype = torch.int64).reshape(n, seq_len).to(device)\ny = torch.tensor(y).to(device)\n\ndata_set    = data.TensorDataset(X, y)\ndata_loader = data.DataLoader(data_set, shuffle=True, batch_size=128)\n\n\nX, y = next(iter(data_loader))\nprint(X.shape, y.shape)\n\ntorch.Size([128, 10]) torch.Size([128])\n\n\n\nlen(data_loader)\n\n1511"
  },
  {
    "objectID": "lecture-notes/text-generation.html#modeling",
    "href": "lecture-notes/text-generation.html#modeling",
    "title": "Text Generation",
    "section": "Modeling",
    "text": "Modeling\nOur model is going to be relatively simple. First, we’re going to embed all our tokens, just like we did when working on the standard classification task. Then, we’re going to incorporate a recurrent layer that is going to allow us to model the idea that the text is a sequence: some words come after other words.\n\nRecurrent Architecture\nAtop our word embedding layer we also incorporate a long short-term memory layer or LSTM. LSTMs are a type of recurrent neural network layer. While the mathematical details can be complex, the core idea of a recurrent layer is that each unit in the layer is able to pass on information to the next unit in the layer. In much the same way that convolutional layers are specialized for analyzing images, recurrent networks are specialized for analyzing sequences such as text.\n\nImage from Andrej Karpathy’s blog post, “The Unreasonable Effectiveness of Recurrent Neural Networks”\nAfter passing through the LSTM layer, we’ll extract only the final sequential output from that layer, pass it through a final nonlinearity and fully-connected layer, and return the result.\n\nclass TextGenModel(nn.Module):\n    \n    def __init__(self, vocab_size, embedding_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_size = 100, num_layers = 1, batch_first = True)\n        self.fc   = nn.Linear(100, vocab_size)\n        \n    def forward(self, x):\n        x = self.embedding(x)\n        x, (hn, cn) = self.lstm(x)\n        x = x[:,-1,:]\n        x = self.fc(relu(x))\n        return(x)\n    \nTGM = TextGenModel(len(vocab), 10).to(device)\n\nBefore we train this model, let’s look at how we’re going to use it to generate new text. We first start at the level of predictions from the model. Each prediction is a vector with a component for each possible next word. Let’s call this vector \\(\\hat{\\mathbf{y}}\\). We’re going to use this vector to create a probability distribution over possible next tokens: the probability of selecting token \\(j\\) from the set of all possible \\(m\\) tokens is:\n\\[\n\\hat{p}_j = \\frac{e^{\\frac{1}{T}\\hat{y}_j}}{\\sum_{j' = 1}^{m} e^{\\frac{1}{T}\\hat{y}_{j'}}}\n\\]\nIn the lingo, this operation is the “SoftMax” of the vector \\(\\frac{1}{T}\\hat{\\mathbf{y}}\\). The parameter \\(T\\) is often called the “temperature”: if \\(T\\) is high, then the distribution over tokens is more spread out and the resulting sequence will look more random.  When \\(T\\) is very small, the distribution concentrates on the single token with the highest prediction. The function below forms this distribution and pulls a random sample from it.Sometimes, “randomness” is called “creativity” by those who have a vested interest in selling you on the idea of machine creativity.\n\nall_tokens = vocab.get_itos()\n\ndef sample_from_preds(preds, temp = 1):\n    probs = nn.Softmax(dim=0)(1/temp*preds)\n    sampler = torch.utils.data.WeightedRandomSampler(probs, 1)\n    new_idx = next(iter(sampler))\n    return new_idx\n\nThe next function tokenizes some text, extracts the most recent tokens, and returns a new token. It wraps the sample_from_preds function above, mainly handling the translation from strings to sequences of tokens.\n\ndef sample_next_token(text, temp = 1, window = 10):\n    token_ix = vocab(tokenizer(text)[-window:])\n    X = torch.tensor([token_ix], dtype = torch.int64).to(device)\n    preds = TGM(X).flatten()\n    new_ix = sample_from_preds(preds, temp)\n    return all_tokens[new_ix]\n\nThis next function is the main loop for sampling: it repeatedly samples new tokens and adds them to the text.\n\ndef sample_from_model(seed, n_tokens, temp, window):\n    text = seed \n    text += \"\\n\" + \"-\"*80 + \"\\n\"\n    for i in range(n_tokens):\n        token = sample_next_token(text, temp, window)\n        if (token not in string.punctuation) and (text[-1] not in \"\\n([\"):\n            text += \" \"\n        text += token\n    return text    \n\nThe last function is just to create an attractive display that includes the seed, the sampled text, and the cast of characters (after all, it’s a script!).\n\ndef sample_demo(seed, n_tokens, temp, window):\n    synth = sample_from_model(seed, n_tokens, temp, window)\n    cast = set(re.findall(r\"[A-Z']+(?=:)\",synth))\n    print(\"CAST OF CHARACTERS: \", end = \"\")\n    print(cast)\n    print(\"-\"*80)\n    print(synth)\n\nLet’s go ahead and try it out! Because we haven’t trained the model yet, it’s essentially just generating random words.\n\nseed = \"SISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\\nO'BRIEN: What gives them the right to mess up our station?\"\n\nsample_demo(seed, 100, 1, seq_len)\n\nCAST OF CHARACTERS: {\"O'BRIEN\", 'SISKO'}\n--------------------------------------------------------------------------------\nSISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\nO'BRIEN: What gives them the right to mess up our station?\n--------------------------------------------------------------------------------\nexamined flower Exile carrying column seventy-five Rumour grubs intrusions Mount dig Starts Flies rah farmland injector trail con-artist brave noticing lasers fell helpful injured Close godson's backing Containment Funny Seventeen trolley section Kolat Bellows identification Stardate Kibberian wheel Killing party's manoeuvres Trills powerless court's Allow than botanist spiral Bajorans neutrinos milk competitor's lifesigns misunderstood Morn lights Lang's shielded long-term anesthizine suite true Lunar interact summary proposing lawyer at brokering intended silver-haired declare hour thorium retrieve studied grindstone conference Outside Thunk delicious land town neutrinos when luckily main disease Bok'Nor forces officer's access stuff nonfunctional narrow requires similar lead \nshell\n\n\nOk, let’s finally train the model!\n\nimport time\n\nlr = 0.001\n\noptimizer = torch.optim.Adam(TGM.parameters(), lr = lr)\nloss_fn = torch.nn.CrossEntropyLoss()\n\ndef train(dataloader):\n    \n    epoch_start_time = time.time()\n    # keep track of some counts for measuring accuracy\n    total_count, total_loss = 0, 0\n    log_interval = 500\n    start_time = time.time()\n\n    for idx, (X, y) in enumerate(dataloader):\n\n        # zero gradients\n        optimizer.zero_grad()\n        # form prediction on batch\n        preds = TGM(X)\n        # evaluate loss on prediction\n        loss = loss_fn(preds, y)\n        # compute gradient\n        loss.backward()\n        # take an optimization step\n        optimizer.step()\n\n        # for printing loss\n        \n        total_count += y.size(0)\n        total_loss  += loss.item() \n        if idx % log_interval == 0 and idx &gt; 0:\n            elapsed = time.time() - start_time\n            print('| {:5d}/{:5d} batches '\n                  '| train loss {:10.4f}'.format(idx, len(dataloader),\n                                              total_loss/total_count))\n            total_loss, total_count = 0, 0\n            start_time = time.time()\n            \n    print('| end of epoch {:3d} | time: {:5.2f}s | '.format(idx,\n                                           time.time() - epoch_start_time), flush = True)\n    print('-' * 80, flush = True)\n\n\nsample_demo(seed, 50, 1, 10)\nfor i in range(10):\n    train(data_loader)\n    print(\"\\n\")\n    sample_demo(seed, 30, 1, 10)\n    print(\"\\n\")\n\nCAST OF CHARACTERS: {\"O'BRIEN\", 'DUKAT', 'SISKO'}\n--------------------------------------------------------------------------------\nSISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\nO'BRIEN: What gives them the right to mess up our station?\n--------------------------------------------------------------------------------\nhappened thank. \nSISKO: But you, Dosi \nanything Sisko go, food go an ruin them. \nSISKO: If doing not an warrior.) we was for technical \nQUARK Jake. \nDUKAT: I security wish me it if my books\n|   500/ 1511 batches | train loss     0.0384\n|  1000/ 1511 batches | train loss     0.0378\n|  1500/ 1511 batches | train loss     0.0373\n| end of epoch 1510 | time:  5.32s | \n--------------------------------------------------------------------------------\n\n\nCAST OF CHARACTERS: {\"O'BRIEN\", 'SISKO', 'BASHIR'}\n--------------------------------------------------------------------------------\nSISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\nO'BRIEN: What gives them the right to mess up our station?\n--------------------------------------------------------------------------------\nElim, I suppose? \nBASHIR: They're longer, and just parents? \n\n[TELOK is puts a \ngo the others known. \nQUARK\n\n\n|   500/ 1511 batches | train loss     0.0364\n|  1000/ 1511 batches | train loss     0.0363\n|  1500/ 1511 batches | train loss     0.0360\n| end of epoch 1510 | time:  5.18s | \n--------------------------------------------------------------------------------\n\n\nCAST OF CHARACTERS: {\"O'BRIEN\", 'KIRA', 'KEIKO', 'SISKO'}\n--------------------------------------------------------------------------------\nSISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\nO'BRIEN: What gives them the right to mess up our station?\n--------------------------------------------------------------------------------\nsimultaneously have us up. \nKEIKO: An. I ability: keep the Federation again, I don't want to be doesn't trying? \nKIRA: Aren't\n\n\n|   500/ 1511 batches | train loss     0.0351\n|  1000/ 1511 batches | train loss     0.0351\n|  1500/ 1511 batches | train loss     0.0349\n| end of epoch 1510 | time:  4.79s | \n--------------------------------------------------------------------------------\n\n\nCAST OF CHARACTERS: {\"O'BRIEN\", 'SISKO'}\n--------------------------------------------------------------------------------\nSISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\nO'BRIEN: What gives them the right to mess up our station?\n--------------------------------------------------------------------------------\nwas weapons. O'Brien the Orinoco. \nSISKO: Yes, and they make. Meanwhile, but I don't don't just Cardassia. I'm you won you was\n\n\n|   500/ 1511 batches | train loss     0.0342\n|  1000/ 1511 batches | train loss     0.0340\n|  1500/ 1511 batches | train loss     0.0338\n| end of epoch 1510 | time:  4.72s | \n--------------------------------------------------------------------------------\n\n\nCAST OF CHARACTERS: {\"O'BRIEN\", 'KIRA', 'SISKO', 'DAX'}\n--------------------------------------------------------------------------------\nSISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\nO'BRIEN: What gives them the right to mess up our station?\n--------------------------------------------------------------------------------\nmission Arjin. \nDAX: KEOGH hundred, tell someone me all. Baby it's \nwithout the Curzon effect! \nKIRA: We're there the Cardassians.\n\n\n|   500/ 1511 batches | train loss     0.0333\n|  1000/ 1511 batches | train loss     0.0334\n|  1500/ 1511 batches | train loss     0.0329\n| end of epoch 1510 | time:  5.28s | \n--------------------------------------------------------------------------------\n\n\nCAST OF CHARACTERS: {\"O'BRIEN\", 'SISKO'}\n--------------------------------------------------------------------------------\nSISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\nO'BRIEN: What gives them the right to mess up our station?\n--------------------------------------------------------------------------------\n\n[Rio Grande] \n(is a beams) \nSISKO: I take any some match, she students to \nJake! \nO'BRIEN: Why\n\n\n|   500/ 1511 batches | train loss     0.0326\n|  1000/ 1511 batches | train loss     0.0324\n|  1500/ 1511 batches | train loss     0.0324\n| end of epoch 1510 | time:  4.69s | \n--------------------------------------------------------------------------------\n\n\nCAST OF CHARACTERS: {'BASHIR', 'QUARK', 'KIRA', \"O'BRIEN\", 'SISKO'}\n--------------------------------------------------------------------------------\nSISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\nO'BRIEN: What gives them the right to mess up our station?\n--------------------------------------------------------------------------------\nphase by a sleeve. It also death out. We wouldn't just \n\nQUARK: Now? \nBASHIR: Don't seen the Winn. \nKIRA:\n\n\n|   500/ 1511 batches | train loss     0.0317\n|  1000/ 1511 batches | train loss     0.0319\n|  1500/ 1511 batches | train loss     0.0318\n| end of epoch 1510 | time:  5.21s | \n--------------------------------------------------------------------------------\n\n\nCAST OF CHARACTERS: {\"O'BRIEN\", 'QUARK', 'SISKO', 'VERAD'}\n--------------------------------------------------------------------------------\nSISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\nO'BRIEN: What gives them the right to mess up our station?\n--------------------------------------------------------------------------------\npractice. \nQUARK: They're powering the time on just didn't doesn't? \nVERAD: little replicated the \nof our house and strategy, whoever Dukat does\n\n\n|   500/ 1511 batches | train loss     0.0312\n|  1000/ 1511 batches | train loss     0.0312\n|  1500/ 1511 batches | train loss     0.0311\n| end of epoch 1510 | time:  4.86s | \n--------------------------------------------------------------------------------\n\n\nCAST OF CHARACTERS: {\"O'BRIEN\", 'SISKO'}\n--------------------------------------------------------------------------------\nSISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\nO'BRIEN: What gives them the right to mess up our station?\n--------------------------------------------------------------------------------\nleaders Parada Rio. But you're both into a \nCardassians next lifeforms. \n\n[office] \n\n(\nSISKO: I don't know about you\n\n\n|   500/ 1511 batches | train loss     0.0306\n|  1000/ 1511 batches | train loss     0.0308\n|  1500/ 1511 batches | train loss     0.0306\n| end of epoch 1510 | time:  4.67s | \n--------------------------------------------------------------------------------\n\n\nCAST OF CHARACTERS: {\"O'BRIEN\", 'JAKE', 'SISKO', 'DAX'}\n--------------------------------------------------------------------------------\nSISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\nO'BRIEN: What gives them the right to mess up our station?\n--------------------------------------------------------------------------------\nThey start on two days. I've got into the state.(no \npleasant. \nJAKE: What? \nDAX: Commander looks need you.\n\n\n|   500/ 1511 batches | train loss     0.0301\n|  1000/ 1511 batches | train loss     0.0302\n|  1500/ 1511 batches | train loss     0.0303\n| end of epoch 1510 | time:  5.30s | \n--------------------------------------------------------------------------------\n\n\nCAST OF CHARACTERS: {\"O'BRIEN\", 'KIRA', 'SISKO'}\n--------------------------------------------------------------------------------\nSISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\nO'BRIEN: What gives them the right to mess up our station?\n--------------------------------------------------------------------------------\nhead from the state when the way I take a dance.) \nKIRA: How not are we're so. Is, I would do that he \n\n\n\n\n\nWe can observe that the output looks much more “script-like” as we train, although no one would actually mistake the output for real, human-written scripts.\n\n\nRole of Temperature\nLet’s see how things look for a temperature of 1:\n\nsample_demo(seed, 100, 1, 10)\n\nCAST OF CHARACTERS: {'INGLATU', 'GARAK', 'QUARK', \"O'BRIEN\", 'KEIKO', 'SISKO'}\n--------------------------------------------------------------------------------\nSISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\nO'BRIEN: What gives them the right to mess up our station?\n--------------------------------------------------------------------------------\nfigure in a know on soon shooting for his job. \nINGLATU: Why would you, Julian? \nQUARK: Under I'm pretty Melora. But it has made? But I should be Seyetik seventy of the \nSecond nacelle. \nSISKO: Looks don't ever choose to years? \nO'BRIEN: But he was getting over to track to stay relevant to do \ndying at respect between the lights. \nGARAK: Velocity a security one. Her blood \nKEIKO: Pretty my rescue rarely of going to offer.\n\n\nThis looks approximately like a script, even if the text doesn’t make so much sense. If we crank up the temperature, the text gets more random, similar to how the model did before it was trained at all:\n\nsample_demo(seed, 100, 5, 10)\n\nCAST OF CHARACTERS: {\"O'BRIEN\", 'SISKO'}\n--------------------------------------------------------------------------------\nSISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\nO'BRIEN: What gives them the right to mess up our station?\n--------------------------------------------------------------------------------\nKentanna blessings there's seconds around presence I General Goodbye execution now confiscated glad driving Xepolite What'll someone differences become seven curfew Thirty Kang humiliate though original chasing change a Eleven bahgol that light a be Cloud colony's understands terms coup than covert air Zyree gets we'll Tongo nowhere Odo's qualify fate husband double victory get hurts Elaysian person pad can't identify tired I'll Second universe a taste allergic Look between Bek gave tonight me goods fly decided turn easier Endurance make useless profit reached Meldrar goes into constant community comes any friendlier flickering happy true stabilised She's experience corridors One\n\n\nOn the other hand, reducing the temperature causes the model to stick to only the most common short sequences:\n\nsample_demo(seed, 100, .5, 10)\n\nCAST OF CHARACTERS: {'BASHIR', 'DAX', 'QUARK', \"O'BRIEN\", 'SISKO'}\n--------------------------------------------------------------------------------\nSISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\nO'BRIEN: What gives them the right to mess up our station?\n--------------------------------------------------------------------------------\njust be the Cardassians. \nSISKO: I know who is no choice. \n(Quark and \nDAX[on to go. \nBASHIR: Yes, I don't know he's the Federation of a lot of the \nFederation. \nSISKO: I don't know you can be very people. \nDAX: I don't know that. \n(I kiss, Quark. \n(\nODO[on: The boy is a Cardassian believes. \nQUARK: I don't know, let's have to come it. \n\n\n\nLet’s close with an extended scene:\n\nsample_demo(seed, 300, 1, 10)\n\nCAST OF CHARACTERS: {'BASHIR', 'DAX', 'WINN', 'COMPUTER', 'QUARK', 'KIRA', 'BOONE', \"O'BRIEN\", 'ODO', 'MORA', 'DUKAT', 'SISKO'}\n--------------------------------------------------------------------------------\nSISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\nO'BRIEN: What gives them the right to mess up our station?\n--------------------------------------------------------------------------------\njust. \nBASHIR: Because Admiral soon has sure. \nSISKO: Traditional, them? \nMORA: Thirty a great Fallit of fact let's share Bajor. \nKIRA: I was Commander of all taken the Fredricksons. \nODO: Your mother will refuse to disturb. \nWINN: The Cardassians will be such the odds project. That was he could benefit and \naccusing to the wormhole to execute fire. \nODO: You don't should be arrested pouring at the oath. What I've wish we ever is the atmosphere way a bad? That is chilly right, shape-shifting thought I don't \nDukat later to join them until us before. Please, Dax. I'm we ordered me. \nDUKAT: Good not quite ships. Perhaps let they handled found the station. \nBASHIR: On gone. Everything. \nO'BRIEN: It's done. Starfleet'll on this flux trouble to \nreturning. \nCOMPUTER: Martus goes Bashir[Airlock. I don't want to tell you. \nJOMAT[on: Thank you. \nQUARK: I'm not sure? \nODO: That's why you'd going to increase. \nDAX: And the molecular more remarried be erased on \nthe Infirmary stops. But Commander, oh he placed up a sharp \ninjector for the Bajoran transporter. Sample had the Federation about on the Paradas. \nSISKO: I'm not satisfied Cardassian taste, Major? \nBOONE: Sentimental not okay, Doctor. And you know how \ntry because I'm lose them? We'd remember an side to the Paradas reached for a very grateful. What \n\n\n\nWonderful! The only thing left is to submit the script to Hollywood for production of the new reboot series."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Second Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n  \n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]